{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RSw-8WQLaoK",
        "outputId": "9b6bbe98-1200-421c-fe2c-b9aa29296ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      Sampling1_RUS Sampling2_ROS Sampling3_SMOTE  \\\n",
            "M1_LogisticRegression          25.0         93.46           90.85   \n",
            "M2_DecisionTree                75.0         99.35           98.04   \n",
            "M3_RandomForest                50.0         100.0           99.35   \n",
            "M4_KNN                         25.0         98.69           85.29   \n",
            "M5_SVM                          0.0         66.99           68.63   \n",
            "\n",
            "                      Sampling4_Tomek Sampling5_SMOTETomek  \n",
            "M1_LogisticRegression           98.69                92.28  \n",
            "M2_DecisionTree                 98.04                97.99  \n",
            "M3_RandomForest                 98.69                99.66  \n",
            "M4_KNN                          98.69                 84.9  \n",
            "M5_SVM                          98.69                67.45  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "df = pd.read_csv(\"/content/Creditcard_data.csv\")\n",
        "\n",
        "X = df.drop(\"Class\", axis=1)\n",
        "y = df[\"Class\"]\n",
        "\n",
        "sampling_techniques = {\n",
        "    \"Sampling1_RUS\": RandomUnderSampler(random_state=42),\n",
        "    \"Sampling2_ROS\": RandomOverSampler(random_state=42),\n",
        "    \"Sampling3_SMOTE\": SMOTE(random_state=42),\n",
        "    \"Sampling4_Tomek\": TomekLinks(),\n",
        "    \"Sampling5_SMOTETomek\": SMOTETomek(random_state=42)\n",
        "}\n",
        "\n",
        "models = {\n",
        "    \"M1_LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "    \"M2_DecisionTree\": DecisionTreeClassifier(),\n",
        "    \"M3_RandomForest\": RandomForestClassifier(),\n",
        "    \"M4_KNN\": KNeighborsClassifier(),\n",
        "    \"M5_SVM\": SVC()\n",
        "}\n",
        "\n",
        "results = pd.DataFrame(index=models.keys(), columns=sampling_techniques.keys())\n",
        "\n",
        "for s_name, sampler in sampling_techniques.items():\n",
        "    X_res, y_res = sampler.fit_resample(X, y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_res, y_res, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    for m_name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred) * 100\n",
        "        results.loc[m_name, s_name] = round(acc, 2)\n",
        "\n",
        "print(results)\n"
      ]
    }
  ]
}